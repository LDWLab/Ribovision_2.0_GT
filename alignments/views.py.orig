import re, os, warnings, io, base64, json
import datetime
import urllib.request
from subprocess import Popen, PIPE
from Bio import AlignIO, BiopythonDeprecationWarning
from io import StringIO

from django.shortcuts import render
from django.http import HttpResponse, JsonResponse, HttpResponseServerError
from django.urls import reverse
from django.contrib.sites.shortcuts import get_current_site

from alignments.models import *
from alignments.taxonomy_views import *
from alignments.residue_api import *
from alignments.structure_api import *
from alignments.fold_api import *
import alignments.alignment_query_and_build as aqab
from TwinCons.bin.TwinCons import slice_by_name


def trim_alignment(concat_fasta, filter_strain):
    '''Reads a fasta string into alignment and trims it down by filter sequence'''
    from alignments.Shannon import species_index_to_aln_index, truncate_aln
    from Bio import AlignIO
    from io import StringIO
    alignment = list(AlignIO.parse(StringIO(concat_fasta), 'fasta'))[0]
    aln_anchor_map, anchor_ix_in_alignment = species_index_to_aln_index(alignment, filter_strain)
    alignment = truncate_aln(alignment, list(aln_anchor_map.keys()), aln_anchor_map=aln_anchor_map)
    return alignment

def calculate_twincons(alignment):
    '''Calculates twincons score given an alignment object.
    Returns data in a list format for the topology viewer'''
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', BiopythonDeprecationWarning)
    from TwinCons.bin import TwinCons
    list_for_phymeas = ['-as',alignment, '-r', '-mx', 'blosum62']
    alnindex_score, sliced_alns, number_of_aligned_positions, gp_mapping = TwinCons.main(list_for_phymeas)
    list_for_topology_viewer = []
    for alnindex in alnindex_score:
        list_for_topology_viewer.append([alnindex,alnindex_score[alnindex][0]])
    return list_for_topology_viewer

def upload_custom_data_for_mapping(request):
    if request.method == 'POST' and 'filename' in request.FILES:
        data_pairs = []
        file = request.FILES['filename']
        file_iterator = iter(file)
        while True:
            try:
                entry = file_iterator.__next__().decode().strip().split(',')
                data_pairs.append((int(entry[0]), float(entry[1])))
            except StopIteration:
                break
        request.session['csv'] = data_pairs
    if request.method == 'GET':
        data_pairs = request.session.get('csv')
        return JsonResponse(data_pairs, safe = False)

def api_twc_with_upload(request, anchor_structure):
    #### _____________Transform PDBID to taxid______________ ####
    anchor_taxid = pdbid_to_strainid(anchor_structure)

    fastastring = request.session.get('fasta')
    #print('fastastring:\n' + fastastring)

    concat_fasta = re.sub(r'\\n','\n',fastastring,flags=re.M)
    #### _____________Trim down the alignment______________ ####
    alignment = trim_alignment(concat_fasta, str(anchor_taxid))

    #### _______________Calculate TwinCons_________________ ####
    list_for_topology_viewer = calculate_twincons(alignment)

    return JsonResponse(list_for_topology_viewer, safe = False)

def constructEbiAlignmentString(fasta, ebi_sequence, startIndex):
    now = datetime.datetime.now()
    fileNameSuffix = "_" + str(now.year) + "_" + str(now.month) + "_" + str(now.day) + "_" + str(now.hour) + "_" + str(now.minute) + "_" + str(now.second) + "_" + str(now.microsecond)
    ### BE CAREFUL WHEN MERGING THE FOLLOWING LINES TO PUBLIC; PATHS ARE HARDCODED FOR THE APACHE SERVER ###
    alignmentFileName = "/home/Desire-DEV/PVDev/static/alignment" + fileNameSuffix + ".txt"
    ebiFileName = "/home/Desire-DEV/PVDev/static/ebi_sequence" + fileNameSuffix + ".txt"
    mappingFileName = ebiFileName + ".map"
    fasta = re.sub('>Structure sequence[\s\S]*?>','>',fasta)
    fh = open(alignmentFileName, "w")
    fh.write(fasta)
    fh.close()

    fh = open(ebiFileName, "w")
    fh.write(">Structure sequence\n")
    fh.write(ebi_sequence)
    fh.close()

    shiftIndexBy = 0
    if startIndex > 1:
        shiftIndexBy = startIndex - 1

    pipe = Popen("mafft --quiet --addfull " + ebiFileName + " --mapout " + alignmentFileName + "; cat " + mappingFileName, stdout=PIPE, shell=True)
    output = pipe.communicate()[0]

    decoded_text = output.decode("ascii")
    
    if len(decoded_text) <= 0:
        for removeFile in [alignmentFileName, ebiFileName]:
            os.remove(removeFile)
        return HttpResponseServerError("Failed mapping the polymer sequence to the alignment!\nTry a different structure.")

    text = decoded_text.split('\n#')[1]
    amendedAln = re.sub('>Structure sequence$','',decoded_text.split('\n#')[0])
    outputDict, mapping, firstLine, badMapping = dict(), dict(), True, 0
    for line in text.split('\n'):
        if firstLine:
            firstLine = False
            continue
        row = line.split(', ')
        if len(row) < 3:
            continue
        if row[2] == '-':
            badMapping += 1
            continue
        if row[1] == '-':
            return HttpResponseServerError("Failed mapping the polymer sequence to the alignment!\nTry a different structure.")
        mapping[int(row[2])] = int(row[1]) + shiftIndexBy

    outputDict["structureMapping"] = mapping
    if badMapping > 0:
        outputDict['BadMappingPositions'] = badMapping

    for removeFile in [alignmentFileName, ebiFileName, mappingFileName]:
        os.remove(removeFile)
    outputDict["amendedAln"] = f'>Structure sequence{amendedAln.split(">Structure sequence")[1]}{amendedAln.split(">Structure sequence")[0]}'
    return outputDict

def request_post_data(post_data):
    fasta = post_data["fasta"]
    ebi_sequence = post_data["ebi_sequence"]
    startIndex = int(post_data["startIndex"])
    return fasta, ebi_sequence, startIndex

def make_map_from_alnix_to_sequenceix(request):
    fasta, ebi_sequence, startIndex = request_post_data(request.POST)
    mapping = constructEbiAlignmentString(fasta, ebi_sequence, startIndex)
    if type(mapping) != dict:
        return mapping
    return JsonResponse(mapping, safe = False)

def api_twc_parameterless(request):
    fasta = request.POST["fasta"]
    concat_fasta = re.sub(r'\\n','\n', fasta,flags=re.M)
    list_for_topology_viewer = calculate_twincons(concat_fasta)
    return JsonResponse(list_for_topology_viewer, safe = False)

def api_twc(request, align_name, tax_group1, tax_group2, anchor_structure=''):

    #### _____________Transform PDBID to taxid______________ ####
    if anchor_structure != '':
        anchor_taxid = pdbid_to_strainid(anchor_structure)
        filter_strain = str(Species.objects.filter(strain_id = anchor_taxid)[0].strain).replace(" ", "_")

    fastastring = request.POST.get('fasta')
    if fastastring is None:
        #### _________Query database for the alignment__________ ####
        align_id = Alignment.objects.filter(name = align_name)[0].aln_id

        rawsqls = []
        for parent in [tax_group1, tax_group2]:
            rawsqls.append((aqab.sql_filtered_aln_query(align_id, parent), Taxgroups.objects.get(pk=parent).groupname))
        nogap_tupaln = dict()
        max_alnposition = 0
        for rawsql, parent in rawsqls:
            nogap_tupaln, max_alnposition= aqab.query_to_dict_structure(rawsql, parent, nogap_tupaln, max_alnposition)

        print(nogap_tupaln)
        #### __________________Build alignment__________________ ####
        fastastring, frequency_list = aqab.build_alignment_from_multiple_alignment_queries(nogap_tupaln, max_alnposition)
    
    concat_fasta = re.sub(r'\\n','\n',fastastring,flags=re.M)
    #print(concat_fasta)
    
    #### ______________Trim down the alignment______________ ####
    if anchor_structure != '':
        alignment = trim_alignment(concat_fasta, filter_strain)
    else:
        alignment = concat_fasta
    
    #### ________________Calculate TwinCons_________________ ####
    list_for_topology_viewer = calculate_twincons(alignment)
    
    return JsonResponse(list_for_topology_viewer, safe = False)

def minmaxIndex_handler(minIndex, maxIndex):
    if (minIndex == ''):
        minIndex = str(0)
    else:
        minIndex = str(minIndex)
    if (maxIndex == ''):
        maxIndex = str(100000)
    else:
        maxIndex = str(maxIndex)
    return minIndex, maxIndex

def twincons_handler(request, anchor_structure, chain, align_name='', tax_group1='', tax_group2='', minIndex = '', maxIndex = ''):
    from django.urls import resolve
    current_url = resolve(request.path_info).url_name
    minIndex, maxIndex = minmaxIndex_handler(minIndex, maxIndex)
    context = dict()
    context = {
        'pdbid': anchor_structure, 
        'chainid': chain,
        'minIndex' : minIndex,
        'maxIndex' : maxIndex
    }
    if current_url == 'twc_with_upload':
        context['entropy_address'] = "upload/twc-api/"+str(anchor_structure)
    elif current_url == 'custom_csv_data_viewer':
        upload_custom_data_for_mapping(request)
        context['entropy_address'] = "custom-csv-data"
    elif current_url == 'twincons':
        context['entropy_address'] = "twc-api/"+align_name+"/"+str(tax_group1)+"/"+str(tax_group2)+"/"+str(anchor_structure)
    return render(request, 'alignments/twc_detail.html', context)

def entropy(request, align_name, tax_group, anchor_structure):
    from alignments import Shannon
    taxid = pdbid_to_strainid(anchor_structure)
    filter_strain = Species.objects.filter(strain_id = taxid)[0].strain
    align_id = Alignment.objects.filter(name = align_name)[0].aln_id
    polymerid = PolymerData.objects.values("pdata_id").filter(polymeralignments__aln = align_id, strain = taxid)[0]["pdata_id"]
    chainid = Chainlist.objects.values("chainname").filter(polymer = polymerid)[0]["chainname"]
    
    rawsql_result = aqab.sql_filtered_aln_query(align_id, tax_group)
    nogap_tupaln, max_aln_length = aqab.query_to_dict_structure(rawsql_result, Taxgroups.objects.get(pk=tax_group).groupname)
    fastastring, frequency_list = aqab.build_alignment_from_multiple_alignment_queries(nogap_tupaln, max_aln_length)
    #fastastring,max_aln_length = aqab.sql_filtered_aln_query(align_id,tax_group)
    aln_shannon_list = Shannon.main(['-a',fastastring,'-f','fastastring','--return_within','-s',filter_strain])
    #print(aln_shannon_list)
    context = {
        'pdbid': anchor_structure, 
        'chainid': chainid, 
        'shannon_dictionary': aln_shannon_list, 
        'entropy_address':"entropy-api/"+align_name+"/"+str(tax_group)+"/"+str(anchor_structure)
    }
    return render(request, 'alignments/entropy_detail.html', context)

def api_entropy(request, align_name, tax_group, anchor_structure):
    from alignments import Shannon
    import os
    from django.conf import settings
    taxid = pdbid_to_strainid(anchor_structure)
    filter_strain = Species.objects.filter(strain_id = taxid)[0].strain
    align_id = Alignment.objects.filter(name = align_name)[0].aln_id
    
    rawsql_result = aqab.sql_filtered_aln_query(align_id, tax_group)
    nogap_tupaln, max_aln_length = aqab.query_to_dict_structure(rawsql_result, Taxgroups.objects.get(pk=tax_group).groupname)
    fastastring, frequency_list = aqab.build_alignment_from_multiple_alignment_queries(nogap_tupaln, max_aln_length)
    #fastastring,max_aln_length = aqab.sql_filtered_aln_query(align_id,tax_group)
    aln_shannon_list = Shannon.main(['-a',fastastring,'-f','fastastring','--return_within','-s',filter_strain])
    return JsonResponse(aln_shannon_list, safe = False)

def index_test(request):
    some_Alignments = Alignment.objects.all()
    superKingdoms = Taxgroups.objects.raw('SELECT * FROM TaxGroups WHERE\
         TaxGroups.groupLevel = "superkingdom";')
    
    context = {
        'props': list(Taxgroups.objects.values('taxgroup_id', 'groupname')),
        'some_Alignments': some_Alignments,
        'superKingdoms': superKingdoms
    }
    return render(request, 'alignments/index_test.html', context)

def desireAPI (request):
    return render(request, 'alignments/desireAPIindex.html')

def index_orthologs(request):
    print ("request.method == 'POST': " + str(request.method == 'POST'))
    print ("'custom_propensity_data' in request.FILES: " + str('custom_propensity_data' in request.FILES))
    print ("request.method: " + str(request.method))
    for x in request.FILES:
        print ("x: " + str(x))
    if request.method == 'GET' and 'custom_propensity_data' in request.FILES:
        propensity_indices_file = request.FILES['custom_propensity_data']
        propensity_indices_string = ''
        for propensity_part in propensity_indices_file.chunks():
            propensity_indices_string += propensity_part.decode()
        print ("propensity_indices_string: " + propensity_indices_string)
    # if request.method == 'POST' and 'custom_propensity_data' in request.FILES:
    #     propensity_indices_file = request.FILES['custom_propensity_data']
    #     propensity_indices_string = ''
    #     for propensity_part in propensity_indices_file.chunks():
    #         propensity_indices_string += propensity_part.decode()
    #     print ("propensity_indices_string: " + propensity_indices_string)
    return render(request, 'alignments/index_orthologs.html')
def index(request):
    return render(request, 'alignments/index.html')

def visualizer(request, align_name, tax_group1, tax_group2, anchor_structure = ''):
    twc_api_url = "http://127.0.0.1:8000/orthologs/twc-api/" + align_name + "/" + str(tax_group1) + "/" + str(tax_group2) + "/" + anchor_structure
    context = {
        "twc_api_url" : twc_api_url
    }
    return render(request, 'alignments/simpleVisualization.html', context)
    #return visualizerHelper(request, align_name + "/" + str(tax_group1) + "/" + str(tax_group2) + "/" + anchor_structure)

def upload_custom_data(request):
    return render(request, 'alignments/upload_custom_data.html')

def paralog_entry_form(request):
    return render(request, 'alignments/index_paralogs.html')

def paralog_display_entropy(request, align_name, fold1, fold2):
    pass
    #return render(request, 'alignments/twc_detail.html', context)

def extract_species_list(fastastring):
    '''Filters out species from a fastastring'''
    unf_species_list = [x.split('\\')[0] for x in fastastring.split('>')[1:]]
    filtered_spec_list = [re.sub('_',' ', re.sub(r'^.*?_', '', x)) for x in unf_species_list]
    return filtered_spec_list

def extract_gap_only_cols(fastastring):
    '''Extracts positions in the fastastring that are only gaps'''
    unf_seq_list = [x.split('\\n')[1] for x in fastastring.split('>')[1:]]
    list_for_intersect = list()
    for sequence in unf_seq_list:
        iterator = re.finditer('-', sequence)
        gap_positions = [m.start(0) for m in iterator]
        list_for_intersect.append(gap_positions)
    gap_only_cols = list(set(list_for_intersect[0]).intersection(*list_for_intersect))
    return gap_only_cols

def construct_dict_for_json_response(response_data):
    '''Takes list of datas for Json response.
    Check their types and assigns names to each.
    Returns them as a dictionary.'''
    response_dict = dict()
    for entry in response_data:
        if type(entry) == str:
            response_dict['Alignment'] = entry
            continue
        if type(entry) == bool:
            response_dict['TwinCons'] = entry
            continue
        if type(entry) == list:
            if all(isinstance(item, int) for item in entry):
                response_dict['Gap-only columns'] = entry
                continue
            if all(isinstance(item, list) for item in entry):
                response_dict['AA frequencies'] = entry
                continue
            if all(isinstance(item, str) for item in entry):
                response_dict['Sequence names'] = entry
                continue
    return response_dict

def simple_fasta(request, aln_id, tax_group, internal=False):
    rawsqls = []
    if type(tax_group) == int:
        tax_group = str(tax_group)
    for parent in tax_group.split(','):
        rawsqls.append((aqab.sql_filtered_aln_query(aln_id, parent), Taxgroups.objects.get(pk=parent).groupname))

    nogap_tupaln = dict()
    max_alnposition = 0

    for rawsql, parent in rawsqls:
        nogap_tupaln, max_alnposition= aqab.query_to_dict_structure(rawsql, parent, nogap_tupaln, max_alnposition)
    
    fastastring, frequency_list = aqab.build_alignment_from_multiple_alignment_queries(nogap_tupaln, max_alnposition)
    
    if internal:
        return fastastring
    
    concat_fasta, twc, gap_only_cols, filtered_spec_list, alignment_obj = calculateFastaProps(fastastring)
    response_dict = construct_dict_for_json_response([concat_fasta,filtered_spec_list,gap_only_cols,frequency_list,twc])

    return JsonResponse(response_dict, safe = False)

def calculateFastaProps(fastastring):
    concat_fasta = re.sub(r'\\n','\n',fastastring,flags=re.M)
    alignment_obj = AlignIO.read(StringIO(concat_fasta), 'fasta')
    twc = False
    if (len(alignment_obj) < 1000):
        sliced_alns = slice_by_name(alignment_obj)
        if len(sliced_alns.keys()) == 2:
            twc = True
    gap_only_cols = extract_gap_only_cols(fastastring)
    filtered_spec_list = extract_species_list(fastastring)
    return concat_fasta, twc, gap_only_cols, filtered_spec_list, alignment_obj

def rProtein(request, align_name, tax_group):
    #if tax_group == 0 - no filter
    align_id = Alignment.objects.filter(name = align_name)[0].aln_id
    fastastring = simple_fasta(request, align_id, tax_group, internal=True)
    print(fastastring)
    #fastastring,max_aln_length = aqab.sql_filtered_aln_query(align_id,tax_group)
    context = {'fastastring': fastastring, 'aln_name':str(Alignment.objects.filter(aln_id = align_id)[0].name)}
    return render(request, 'alignments/detail.html', context)

def rRNA(request, align_name, tax_group):
    from Bio.SeqUtils import IUPACData
    align_id = Alignment.objects.filter(name = align_name)[0].aln_id
    rawsql_result = aqab.sql_filtered_aln_query(align_id, tax_group)
    nogap_tupaln = dict()
    nogap_tupaln, max_aln_length = aqab.query_to_dict_structure(rawsql_result, Taxgroups.objects.get(pk=tax_group).groupname, nogap_tupaln)
    fastastring, frequency_list = aqab.build_alignment_from_multiple_alignment_queries(nogap_tupaln, max_aln_length, IUPACData.unambiguous_rna_letters)
    #fastastring,max_aln_length = aqab.sql_filtered_aln_query(align_id,tax_group)
    context = {'fastastring': fastastring, 'aln_name':str(Alignment.objects.filter(aln_id = align_id)[0].name)}
    return render(request, 'alignments/rRNA.html', context)

def validate_fasta_string(fastaString):
    malicious_strings = [
        "eval\(unescape",
        "base64_decode\(",
        "substr\(md5\(strrev\(",
        "cwd = @getcwd\(\);",
        "chr\(\(ord\(",
        "gzinflate\(base64_decode\(",
        "php_uname\(\)\" \"] = chr\(ord\(",
        "cwd\[strlen\(\$cwd\)",
        "ini_get\('safe_mode'\);",
        "=\"\x62\"",
        "\"+ r + \"&r=\" + document.referrer;\"",
        "if\(strtoupper\(substr\(PHP_OS, 0, 3\) \) == \"WIN\"\)",
        "window.top.location.href=\"http://",
        "@ini_get\(\"disable_functions\"\)",
        "\$g3='';\$g3.=\$r;\$g3.=\$h;\$g3.=\$y",
    ]
    for mstring in malicious_strings:
        regex = re.compile(mstring)
        if re.search(regex, fastaString):
            return False
    return True

# trims fasta by a list of indices
def trim_fasta_by_index(input_file, indices):
    from Bio import AlignIO
    align = AlignIO.read(input_file, "fasta")
    trimmed_align = align[:,int(indices[0]):int(indices[0])] # initialize align object
    for i in indices.split(','):
        if (int(i)-1 < 0):
            continue
        trimmed_align += align[:,int(i)-1:int(i)]
    return trimmed_align

def propensity_data_custom (request):
    response = propensity_data(request, None, None)
    return response

def propensity_data(request, aln_id, tax_group):
    from io import StringIO
    import alignments.propensities as propensities

    if request.method == 'POST' and 'customFasta' in request.POST and aln_id is None:
        fastastring = request.POST['customFasta']
    else:
        fastastring = simple_fasta(request, aln_id, tax_group, internal=True).replace('\\n', '\n')
    fasta = StringIO(fastastring)

    if request.method == 'POST' and 'indices' in request.POST:
        indices = request.POST['indices']
        trimmed_fasta = trim_fasta_by_index(fasta, indices)
        fasta = StringIO(format(trimmed_fasta, 'fasta'))

    aa = propensities.aa_composition(fasta, reduced = False)
    fasta.seek(0) # reload the fasta object
    red_aa = propensities.aa_composition(fasta, reduced = False)

    data = {
        'aln_id' : aln_id,
        'tax_group' : tax_group,
        'reduced alphabet' : red_aa,
        'amino acid' : aa
    }
    return JsonResponse(data)

def permutation_data_custom(request):
    return permutation_data(request, None, None)

def permutation_data(request, aln_id, tax_group):
    from io import StringIO
    from Bio import AlignIO
    # if request.method == 'POST' and 'customFasta' in request.POST and aln_id is None:
    #     fastastring = request.POST['customFasta']
    # else:
    #     fastastring = simple_fasta(request, aln_id, tax_group, internal=True).replace('\\n', '\n')
    # fasta = StringIO(fastastring)
    # if request.method == 'POST' and 'indices' in request.POST:
    #     indices = request.POST['indices']
    #     trimmed_fasta = trim_fasta_by_index(fasta, indices)
    #     fasta = StringIO(format(trimmed_fasta, 'fasta'))
    fasta_variable_name = 'customFasta'
    if request.method == 'POST' and fasta_variable_name in request.POST:
        fastastring = request.POST[fasta_variable_name]
        fasta = StringIO(fastastring)
        align = AlignIO.read(fasta, "fasta")
        # column_dimension = align.get_alignment_length()
        # midIndex = column_dimension // 2
        # permutation_index_variable_name = 'permutation_index'
        # if permutation_index_variable_name in request.POST:
        #     midIndex = int(request.POST[permutation_index_variable_name])
        # else:
        #     midIndex = 0
        indices_variable_name = 'indices'
        if indices_variable_name in request.POST:
            indices = request.POST[indices_variable_name]
            indexPairs = indices.replace(" ", "").split(',')
            label_modifiers = []
            per_row_gap_counts = []
            # align = align[0:1, :]
            row_range = range(len(align))
            column_range = range(align.get_alignment_length())
            for row_index in row_range:
                label_modifiers.append("")
                gap_counts_at_row_index = [(0, 0)]
                per_row_gap_counts.append(gap_counts_at_row_index)
                running_concurrent_gap_count = 0
                running_gap_count = 0
                for column_index in column_range:
                    if align[row_index, column_index] == '-':
                        running_concurrent_gap_count += 1
                    elif running_concurrent_gap_count > 0:
                        running_gap_count += running_concurrent_gap_count
                        running_concurrent_gap_count = 0
                        gap_counts_at_row_index.append((column_index, running_gap_count))
            newAlign = align[:, 0:0]

            # recordList = []
            # newAlign = MultipleSeqAlignment(recordList)

            valid_index_pair_flag = False
            for indexPair in indexPairs:
                indexPair = indexPair.split('-')
                column_index_0 = int(indexPair[0]) - 1
                column_index_1 = int(indexPair[1])
                if 0 <= column_index_0 < column_index_1:
                    valid_index_pair_flag = True
                    for row_index in row_range:
                        base_column_index = column_index_0
                        while align[row_index, base_column_index] == '-':
                            base_column_index += 1
                        previous_column_index_gap_flag = False
                        _range = range(base_column_index + 1, column_index_1)
                        for column_index in _range:
                            column_index_gap_flag = align[row_index, column_index] == '-'
                            if column_index_gap_flag != previous_column_index_gap_flag:
                                if column_index_gap_flag:
                                    if column_index == base_column_index + 1:
                                        label_modifiers[row_index] += str(base_column_index + 1) + ", "
                                    else:
                                        label_modifiers[row_index] += str(base_column_index + 1) + "-" + str(column_index + 1) + ", "
                                else:
                                    base_column_index = column_index
                            previous_column_index_gap_flag = column_index_gap_flag
                        if align[row_index, column_index_1 - 1] != '-':
                            if base_column_index == column_index_1 - 1:
                                label_modifiers[row_index] += str(base_column_index + 1) + ", "
                            else:
                                label_modifiers[row_index] += str(base_column_index + 1) + "-" + str(column_index_1) + ", "
                    newAlign += align[:, column_index_0:column_index_1]
                #     for row_index in row_range:
                #         gap_counts_at_row_index = per_row_gap_counts[row_index]
                #         gap_counts_at_row_index_range = range(len(gap_counts_at_row_index))
                #         lower_gap_count_index = 0
                #         for gap_count_index in gap_counts_at_row_index_range:
                #             gap_count = gap_counts_at_row_index[gap_count_index]
                #             gap_count_column_index = gap_count[0]
                #             if gap_count_column_index > column_index_0:
                #                 break
                #             lower_gap_count_index = gap_count_index
                #         upper_gap_count_index = lower_gap_count_index
                #         for gap_count_index in gap_counts_at_row_index_range[lower_gap_count_index + 1:]:
                #             gap_count = gap_counts_at_row_index[gap_count_index]
                #             gap_count_column_index = gap_count[0]
                #             if gap_count_column_index > column_index_1:
                #                 break
                #             upper_gap_count_index = gap_count_index
                        
                #         label_modifiers[row_index] += str(column_index_0 - gap_counts_at_row_index[lower_gap_count_index][1] + 1) + "-" + str(column_index_1 - gap_counts_at_row_index[upper_gap_count_index][1]) + ", "
            if valid_index_pair_flag:
                for row_index in row_range:
                    if len(label_modifiers[row_index]) > 0:
                        newAlign[row_index].id = re.sub('([^_]*)_?$', r'\1_', newAlign[row_index].id) + label_modifiers[row_index][:-2]
                        newAlign[row_index].description = ""
            align = newAlign
        #     minimumIndex = column_dimension
        #     maximumIndex = 0
        #     for index in indices.split(','):
        #         index = int(index)
        #         if (index < minimumIndex):
        #             minimumIndex = index
        #         if (index > maximumIndex):
        #             maximumIndex = index
        #     midIndex = (minimumIndex + maximumIndex) // 2

        # This is the top row of the alignment. It makes for easy checking of the permutation; it serves no other purpose.
        # align = align[0:1, midIndex:] + align[0:1, :midIndex]
        # align = align[:, midIndex:] + align[:, :midIndex]
    else:
        raise NotImplementedError()
    fasta = StringIO(format(align, 'fasta'))
    # lines = fasta.split("\n")
    # linePairs = []
    # labelLine = lines[0]
    # alignmentLine = ""
    # for line in lines[1:]:
    #     if line.startswith('>'):
    #         linePairs.append((labelLine, alignmentLine))
    #         labelLine = line
    #         alignmentLine = ""
    #     else:
    #         alignmentLine += line
    permutation_string = fasta.getvalue()
    # request.FILES['permuted_fasta'] = permutation_string

    now = datetime.datetime.now()
    fileNameSuffix = "_" + str(now.year) + "_" + str(now.month) + "_" + str(now.day) + "_" + str(now.hour) + "_" + str(now.minute) + "_" + str(now.second) + "_" + str(now.microsecond)
    alignmentFileName = "/home/Desire-DEV/PVDev/static/permuted_alignment" + fileNameSuffix + ".fasta"

    # hhblits
    # command: /usr/local/bin/hh-suite/bin/hhsearch -i /home/blastdb/alignments/beta_barrels/OB_aIF1.fa -d /home/blastdb/ecod_F_fasta/ecod215/ecod215_numk3 -maxres 550000 -o OB_aIF1.txt -M 50 -add_cons
    # Convert to standard JSON:
    # NOTE: add any installed prerequisites to the README.md (/DESIRE/README.md)

    fh = open(alignmentFileName, "w")
    fh.write(permutation_string)
    fh.close()

    os.remove(alignmentFileName)

    response = HttpResponse(permutation_string, content_type="text/plain")
    return response
# >Bacteria_Synechococcus_sp._PCC_7335_\n
# NALPLHRIPLGTTVHNVELVPGRGGQVVRAAGAGAQLVAKEGG--YVTLKLPSSEVRMIR\nRECYATIGQVGNVEHRNLSLGKAGRKRWA-------GRRPEVRGSVMNPVDHPHGGGE--\n-GRAPIG-----RSGPVTP-WGKPALGYKTRKKKK----GSDAMIVRRRRRSSKRGRGGR\nNAMGIRSYRPLTPGTRERTV-SDFSTVTADK-PEKSLTYSV-----------HRPKG-RN\nN-RGVITCRHRGGGH-----KRLYR--------EIDFRRN--------KFNVPAKVATIE\nYDPNRNARISLLHYE-DGE-----KRYILHPIGLEVGATIVSG---EDAPFEVG\n


# HRIPLGTTVHNVELVPGRGGQVVRAAGAGAQLVAKEGG--YVTLKLPSSEVRMIRRECYA
# TIGQVGNVEHRNLSLGKAGRKRWA-------GRRPEVRGSVMNPVDHPHGGGE---GRAP
# IG-----RSGPVTP-WGKPALGYKTRKKKK----GSDAMIVRRRRRSSKRGRGGRNAMGI
# RSYRPLTPGTRERTV-SDFSTVTADK-PEKSLTYSV-----------HRPKG-RNN-RGV
# ITCRHRGGGH-----KRLYR--------EIDFRRN--------KFNVPAKVATIEYDPNR
# NARISLLHYE-DGE-----KRYILHPIGLEVGATIVSG---EDAPFEVGNALPL

def propensities(request, align_name, tax_group):
    aln_id = Alignment.objects.filter(name = align_name)[0].aln_id
    propensity_data = reverse('alignments:propensity_data', kwargs={'aln_id': aln_id, 'tax_group' : tax_group})

    names = []
    if type(tax_group) == int:
        tax_group = str(tax_group)
    for group in tax_group.split(','):
        names.append(Taxgroups.objects.get(pk=group).groupname)

    context = {
        "propensity_data" : propensity_data, 
        "align_name" : align_name,
        "tax_name" : ', '.join(names)
    }
    
    return render(request, 'alignments/propensities.html', context)

def flushSession (request):
    try:
        request.session.flush()
    except:
        return HttpResponseServerError ("Failed to flush the session!")
    return HttpResponse ("Success!")

def ecodPassThroughQuery(request):
    '''Request a password protected URL from our website that returns a JSON object.
    '''
    baseURL = 'http://'+get_current_site(request).domain
    url = baseURL+request.GET['url']
    if ('&format=json' not in url):
        url += '&format=json'
    req = urllib.request.Request(url)
    with open('/etc/desire_config.json') as config_file:
        config = json.load(config_file)
    credentials = (f'{config["DB_USER_NAME"]}:{config["DB_PASSWORD"]}')
    encoded_credentials = base64.b64encode(credentials.encode('ascii'))
    req.add_header('Authorization', 'Basic %s' % encoded_credentials.decode("ascii"))

    response = urllib.request.urlopen(req)
    encoding = response.info().get_content_charset('utf-8')
    data = response.read()
    return JsonResponse(json.loads(data.decode(encoding)), safe=False)


def parse_string_structure(stringData, strucID):
    from Bio.PDB import MMCIFParser
    parser = MMCIFParser()
    strucFile = io.StringIO(stringData)
    structureObj = parser.get_structure(strucID,strucFile)
    return structureObj
